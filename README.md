# KNN Classification and Regression on AI Kosh Dataset

This project demonstrates how to apply **K-Nearest Neighbors (KNN)** for both:

- âœ… Classification â€” predicting disability type  
- âœ… Regression â€” predicting total population count  

using a real-world dataset from Indiaâ€™s **AI Kosh** platform.

The impact of varying the value of **K** on model performance is analyzed through:

- ðŸ“ˆ Accuracy vs K (Classification)
- ðŸ“‰ RMSE vs K (Regression)

---

## ðŸ“‚ Dataset

Source: AI Kosh â€“ UDID Disability Statistics Dataset  

The dataset contains:

- **Categorical Features**
  - state_name
  - district_name
  - age_group

- **Numerical Features**
  - male_count
  - female_count

- **Targets**
  - disability_type_name (classification)
  - total_count (regression)

> âš ï¸ The dataset is not included in this repository.  
> Please download it from the AI Kosh portal and place it in the project directory as:
>
> `UDIDDATA_0.csv`

---

## ðŸ§  Methodology

1. Load and inspect dataset
2. Handle missing values (if any)
3. Split features and targets
4. Encode categorical variables using One-Hot Encoding
5. Scale numerical features using StandardScaler
6. Train KNN models for multiple K values
7. Evaluate:
   - Accuracy for classification
   - RMSE for regression
8. Plot performance curves

---

## ðŸ“Š Results Summary

- **Classification:** Best accuracy observed around K = 9â€“11  
- **Regression:** Lowest RMSE observed around K = 1â€“3  

These results show how neighborhood size strongly influences KNN behavior.

---



```bash
pip install pandas numpy matplotlib scikit-learn
